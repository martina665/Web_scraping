{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database: super fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = \"https://www.databaseitalia.it/category/politica/\"\n",
    "soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "all_data = []\n",
    "for title in tqdm(soup.select(\"h2.post-title\")):     \n",
    "    t = title.get_text(strip=True)\n",
    "    u = title.a[\"href\"]\n",
    "    s = BeautifulSoup(\n",
    "        requests.get(\"https://www.databaseitalia.it/\"+ u).content, \"html.parser\"\n",
    "    )\n",
    "  \n",
    "    text= s.find_all('p')\n",
    "    #text = text.get_text(strip=True, separator=\"\\n\") if text else \"\"\n",
    "    all_data.append([t, text, u])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=[\"Title\", \"content\", \"PageLink\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenari economici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://scenarieconomici.it/\"\n",
    "soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "all_data = []\n",
    "for title in tqdm(soup.select(\"h2.entry-title\")):\n",
    "    t = title.get_text(strip=True)\n",
    "    u = title.a[\"href\"]\n",
    "    s = BeautifulSoup(\n",
    "        requests.get( u).content, \"html.parser\"\n",
    "    )\n",
    "    text = s.select_one('[itemprop=\"articleBody\"]')\n",
    "    text = text.get_text(strip=True, separator=\"\\n\") if text else \"\"\n",
    "\n",
    "    all_data.append([t, text, u])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=[\"Title\", \"content\", \"PageLink\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devi informarti : blog inattivo (spero) dal 2k18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"http://devinformarti.blogspot.com/\"\n",
    "soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "all_data = []\n",
    "for title in tqdm(soup.select(\"h3.post-title\")):\n",
    "    t = title.get_text(strip=True)\n",
    "    u = title.a[\"href\"]\n",
    "    s = BeautifulSoup(\n",
    "        requests.get( u).content, \"html.parser\"\n",
    "    )\n",
    "    text= s.find('div',{'class':'post-body entry-content float-container'}).text.strip()\n",
    "    #text = s.select_one('[div=\"post-body entry-content float-container\"]')\n",
    "    #text = text(strip=True, separator=\"\\n\") if text else \"\"\n",
    "\n",
    "    all_data.append([t, text, u])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=[\"Title\", \"content\", \"PageLink\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['generico'] #qui cambia always\n",
    "df2 = df.assign(category=cat*11)\n",
    "import time\n",
    "from datetime import datetime\n",
    "dat = datetime.now()\n",
    "df3 = df2.assign(Date=dat)\n",
    "df3.to_csv('C:/Users/marti/OneDrive/Desktop/news/csvfiles/fakenews/27-07-2022_devi-inf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imola oggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.imolaoggi.it/category/econom/\"\n",
    "soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "all_data = []\n",
    "for title in tqdm(soup.select(\"h3.entry-title\")):\n",
    "    t = title.get_text(strip=True)\n",
    "    u = title.a[\"href\"]\n",
    "    s = BeautifulSoup(\n",
    "        requests.get( u).content, \"html.parser\"\n",
    "    )\n",
    "    text= s.find('div',{'class':'entry-content'}).text.strip() \n",
    "    #text = s.select_one('[div=\"post-body entry-content float-container\"]')\n",
    "    #text = text(strip=True, separator=\"\\n\") if text else \"\"\n",
    "\n",
    "    all_data.append([t, text, u])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=[\"Title\", \"content\", \"PageLink\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per togliere \"\\n\"\n",
    "df['content']= df['content'].replace('\\n',' ', regex=True)\n",
    "df.content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['economia'] #qui cambia always\n",
    "df2 = df.assign(category=cat*12)\n",
    "import time\n",
    "from datetime import datetime\n",
    "dat = datetime.now()\n",
    "df3 = df2.assign(Date=dat)\n",
    "df3.to_csv('C:/Users/marti/OneDrive/Desktop/news/csvfiles/fakenews/27-07-2022_imola-ogg-economia.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vox news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://voxnews.info/\"\n",
    "soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "all_data = []\n",
    "for title in tqdm(soup.select(\"h1.entry-title\")):\n",
    "    t = title.get_text(strip=True)\n",
    "    u = title.a[\"href\"]\n",
    "    s = BeautifulSoup(\n",
    "        requests.get( u).content, \"html.parser\"\n",
    "    )\n",
    "    text= s.find('div',{'class':'entry-content'}).text.strip() \n",
    "    #text = s.select_one('[div=\"post-body entry-content float-container\"]')\n",
    "    #text = text(strip=True, separator=\"\\n\") if text else \"\"\n",
    "\n",
    "    all_data.append([t, text, u])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=[\"Title\", \"content\", \"PageLink\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per togliere \"\\n\"\n",
    "df['content']= df['content'].replace('\\n',' ', regex=True)\n",
    "df.content[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['economia'] #qui cambia always\n",
    "df2 = df.assign(category=cat*12)\n",
    "import time\n",
    "from datetime import datetime\n",
    "dat = datetime.now()\n",
    "df3 = df2.assign(Date=dat)\n",
    "df3.to_csv('C:/Users/marti/OneDrive/Desktop/news/csvfiles/fakenews/27-07-2022_imola-ogg-economia.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7ccb2a435b656723dafd2fb88eb02b6891256ab8b77d2cf3044cea9811453d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
